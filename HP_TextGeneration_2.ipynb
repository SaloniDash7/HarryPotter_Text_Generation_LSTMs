{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Text Generation using LSTM cells. Model trained over Harry Potter and the Order of Phoenix'''\n",
    "\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import io\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "#Target log path\n",
    "logs_path = '/tmp/tensorflow/rnn_words'\n",
    "\n",
    "#Reading and preprocessing data\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.read()\n",
    "    content = content.replace('\\n', '')  #Removing newlines\n",
    "    content = content.replace(\"'\",\"\")\n",
    "    content = re.sub(\"(?<=\\w)(['!?,@.:;-])\", r' \\1', content)  #Spacing out special characters\n",
    "    content = io.StringIO(content).readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))] #Split String into words\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ]) #Reshape into 1-D array\n",
    "    return content\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()  #Returns a dictionary with count of each word arranged in descending order of count\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) #Basically assigns a number to each word \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "train_data=read_data(\"HP.txt\")\n",
    "dictionary, reverse_dictionary = build_dataset(train_data)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "training_iters = 100000\n",
    "display_step = 100\n",
    "batch_size=256\n",
    "time_steps=3\n",
    "in_size=1\n",
    "vocab_size=len(dictionary)\n",
    "num_layers=3\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, time_steps, in_size]) # X_shape = No_of_batches X time_step X no_inputs\n",
    "y = tf.placeholder(\"float\", [batch_size, vocab_size])\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size])) # Weights_shape = hidden_units X vocab_size\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}\n",
    "\n",
    "def LSTM_cell():\n",
    "    cell = tf.contrib.rnn.NASCell(n_hidden, reuse=tf.get_variable_scope().reuse)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.8)\n",
    "\n",
    "    \n",
    "def RNN(X,weights,biases):\n",
    "    #Stacking 3 LSTM cells, each LSTM cell has n_hidden memory/LSTM units\n",
    "    \n",
    "    rnn_cell = rnn.MultiRNNCell([LSTM_cell() for _ in range(num_layers)])\n",
    "    \n",
    "    #Calculate the outputs of the rnn_cell, last_state=outputs[-1]. \n",
    "    outputs,last_state=tf.nn.dynamic_rnn(rnn_cell,X,dtype=tf.float32) #Shape of Outputs=n_batches X time_steps X n_hidden\n",
    "    #Basically you are swapping dimensions 0 and 1. outputs[-1] will give outputs of the last tim estep\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))  \n",
    "    results=tf.matmul(outputs[-1],weights['out'])+biases['out']\n",
    "    \n",
    "    with tf.name_scope('Weights'):\n",
    "            variable_summaries(weights['out'])\n",
    "    \n",
    "    with tf.name_scope('Biases'):\n",
    "            variable_summaries(biases['out'])\n",
    "    \n",
    "    with tf.name_scope('Activations'):\n",
    "            tf.summary.histogram('Activations',results)\n",
    "    return (results)\n",
    "\n",
    "pred=RNN(X,weights,biases)\n",
    "\n",
    "#Cost,optimizer,accuracy\n",
    "with tf.name_scope(\"Cross_Entropy_Cost\"):\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "    tf.summary.scalar('Cross_Entropy_Cost',cost)\n",
    "\n",
    "optimizer=tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar('Accuracy',accuracy)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session(config=config) as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = 0\n",
    "    end_offset = time_steps + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < training_iters:# Generate a minibatch. Add some randomness on selection process.\n",
    "        batch_x=np.zeros((batch_size,time_steps,in_size))\n",
    "        batch_y=np.zeros((batch_size,vocab_size))\n",
    "        for batch in range(batch_size):\n",
    "            offset = random.randint(0,np.shape(train_data)[0])\n",
    "            if (offset > (len(train_data)-end_offset)):\n",
    "                offset = len(train_data)-end_offset\n",
    "            batch_x[batch]=np.reshape(np.array([ [dictionary[ str(train_data[i])]] for i in range(offset, offset+time_steps) ]),\\\n",
    "                                      [time_steps,in_size])\n",
    "            #One-hot encoding of immediate word\n",
    "            y_onehot = np.zeros([vocab_size], dtype=float)\n",
    "            y_onehot[dictionary[str(train_data[offset+time_steps])]] = 1.0\n",
    "            batch_y[batch]=y_onehot\n",
    "\n",
    "        summary,_, acc, loss, onehot_pred = session.run([merged,optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={X: batch_x, y: batch_y})\n",
    "        writer.add_summary(summary, step)\n",
    "        #Stores total loss and total accuracy in 100 iters \n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step) % display_step == 0:\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            print(\"Iter= \" + str(step) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            \n",
    "            summary, _ = session.run([merged, optimizer],\n",
    "                              feed_dict={X: batch_x, y: batch_y},\n",
    "                              options=run_options,\n",
    "                              run_metadata=run_metadata)\n",
    "            writer.add_run_metadata(run_metadata, 'step%03d' % step)\n",
    "            writer.add_summary(summary, step)\n",
    "            print('Adding run metadata for', step)\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            saver.save(session,\"./saver/model.ckpt\")\n",
    "            print(\"Model saved \")\n",
    "        step += 1\n",
    "        offset += (time_steps+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saver/model.ckpt\n",
      "Optimization Finished!\n",
      "Elapsed time:  27.374006509780884 sec\n",
      "Run on command line.\n",
      "\ttensorboard --logdir=/tmp/tensorflow/rnn_words\n",
      "Point your web browser to: http://localhost:6006/\n",
      "%s words: Harry Potter said\n",
      "Harry Potter said George , looking , said Harry , his Dereliction , and he was not at , I dont , said Harry , trying at the door . Harry was not a Longbottom of the Weasley . He had just the the of tryouts of the Ministry , said Harry , trying at the back of his head , but he had been a , said Hagrid , looking , said Harry . He , who was a passing , said Harry , his a voice , then , said Harry , and he was sure to be , who was\n",
      "%s words: Albus Dumbledore and\n",
      "Albus Dumbledore and the others , said Harry , his eyes , but the door , said Harry , trying , said Harry , trying , said Hermione , looking , said Harry , trying his head . I think think to be , who was now to narrow the door . . . I was - a very , then , said Harry , trying the door . . . . Harry was still to be a long , then you ? said Harry , trying the door , said Harry , trying the door . He was standing to be a\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    saver.restore(session,\"./saver/model.ckpt\")\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    print(\"Run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"Point your web browser to: http://localhost:6006/\")\n",
    "    while True:\n",
    "        prompt = \"%s words: \" \n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        #if len(words) != n_input:\n",
    "            #continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(100):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, time_steps, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={X:keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "        except e:\n",
    "            print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
